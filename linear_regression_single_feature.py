# -*- coding: utf-8 -*-
"""linear_regression_single_feature.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u6uTbwmN2LhXueNwxhRpPKgYrN7p3xiw
"""

import numpy as np

import matplotlib.pyplot as plt

x = [1,2,3,4]
y=[100,150,189,225]
plt.plot(x,y,color ='red',marker='o',linestyle ='dashed',label='test1')
plt.xlabel('features')
plt.ylabel('target')


x1=[4,5,2,1]
y1=[315,330,140,110]

plt.plot(x1,y1,label='test2')
plt.legend()

#plt.scatter(x,y,marker='*')


x = np.array([1,2,3,4])
w,b = 0,0
print(w)
epoch = 100

y=np.array([100,150,170,200])
#print((x@y))

### MSE calculation >  weight bias > weight determination > y_prediction > mse cal > cycle continuatuon********
##


#################            LINEAR REGRESSION WITH MULTIPLE FEATURES AS INPUT AND WEIGHT, BIAS UPDATION            ###############

##<<<<<<<<<<<<<<<<<<                                  MEAN SQUARE ERROR AS LOSS                                     >>>>>>>>>>>>>>>>>>>>>>>>>>


def MSE(y,x,w,b):
  n = len(x)
  y_p =  np.dot(w,x) + b
  y_e= y_p-y
  #print(y_e)
  MSE = (1/len(y))*np.sum((y_p-y)**2)
  print(MSE)
  #print((np.dot(np.sum(y_e,x))))
  w_b = (1/n)*np.sum(np.dot(y_e,(x)))*2
  b_b = (1/n)*np.sum(y_p-y)
  #print(w_b,b_b)
  w = w - 0.1*w_b
  b = b - 0.1*b_b
  return w,b

for i in range(epoch):

  w, b = MSE(y,x,w,b)
  print('epoch=',i,w,b)
y_p = np.dot(w,x)+b
plt.scatter(x,y,marker='*',color='red')
plt.plot(x,y_p,label='vector')
plt.legend()


#y1=np.dot(0.23,x)+0.12

#plt.scatter(x,y,marker='+')
#plt.plot(x,y1,label='svm')

################# LINEAR REGRESSION WITH MULTIPLE FEATURES AS INPUT AND WEIGHT, BIAS UPDATION ###############

##<<<<<<<<<<<<<<<<<<             MEAN SQUARE ERROR AS LOSS           >>>>>>>>>>>>>>>>>>>>>>>>>>

import matplotlib.pyplot as plt


import math

class linear_regression():
  def __init__(self,n_features):
    self.w = np.zeros((n_features, 1))
    self.b =0



  def predict(self,x):
    #print( self.x.shape , self.w.shape)
    #print(y1.shape)
    return np.dot(x,self.w)+b

  def MSE(self,x,y):
    y_pred = self.predict(x)
    MSE = (1/len(y))*np.sum(y_pred-y)**2
    #print(MSE)
    return float(MSE)


  def fit_transform(self,epoch,x,y):
    n = len(y)
    for i in range(epoch):
      MSE = self.MSE(x,y)
      value = float('inf')
      if math.isinf(MSE):
        break
      if i%10 == 0:

        print('EPOCH: ',i,'MSE: ' ,MSE,'weight:',self.w,'bias:',self.b)
      #print( np.transpose(x).shape, (self.predict(x) - y).shape)
      y_pred =self.predict(x)
      self.w_b = (2/n) *np.sum( np.dot(np.transpose(x), (self.predict(x) - y)))
      self.b_b = (2/n )*(y_pred - y)
      self.w -=  0.1 *self.w_b
      self.b -=  0.1 * self.b_b

    return w, b

n_features = int(input('enter no. of features'))

x =np.array([[1,2,3,4],[10,20,30,40]])
y = np.array([[10],
              [22]], dtype=float)
#print(y.shape)
lr = linear_regression(n_features)
w,b = lr.fit_transform(100,x,y)
print(lr.predict(x))
y_pred = lr.predict(x)

print(y_pred.shape,x.shape,y.shape)


plt.plot(x,y, label=['line1','line2','line3','line4'])
plt.plot(x,y_pred)
plt.legend()
plt.ylim( -10,45)

################# LINEAR REGRESSION WITH MULTIPLE FEATURES AS INPUT AND WEIGHT, BIAS UPDATION ###############

##<<<<<<<<<<<<<<<<<<                         LOG LOSS                >>>>>>>>>>>>>>>>>>>>>>>>>>

import matplotlib.pyplot as plt


import math

class linear_regression():
  def __init__(self,n_features):
    self.w = np.zeros((n_features, 1))
    self.b =0



  def predict(self,x):
    #print( self.x.shape , self.w.shape)
    #print(y1.shape)
    return np.dot(x,self.w)+b

  def MSE(self,x,y):
    y_pred = self.predict(x)
    MSE = (1/len(y))*np.sum(y_pred-y)**2
    #print(MSE)
    return float(MSE)


  def fit_transform(self,epoch,x,y):
    n = len(y)
    for i in range(epoch):
      MSE = self.MSE(x,y)
      value = float('inf')
      if math.isinf(MSE):
        break
      if i%10 == 0:

        print('EPOCH: ',i,'MSE: ' ,MSE,'weight:',self.w,'bias:',self.b)
      #print( np.transpose(x).shape, (self.predict(x) - y).shape)
      y_pred =self.predict(x)
      self.w_b = (2/n) *np.sum( np.dot(np.transpose(x), (self.predict(x) - y)))
      self.b_b = (2/n )*(y_pred - y)
      self.w -=  0.1 *self.w_b
      self.b -=  0.1 * self.b_b

    return w, b

n_features = int(input('enter no. of features'))

x =np.array([[1,2,3,4],[10,20,30,40]])
y = np.array([[10],
              [22]], dtype=float)
#print(y.shape)
lr = linear_regression(n_features)
w,b = lr.fit_transform(100,x,y)
print(lr.predict(x))
y_pred = lr.predict(x)

print(y_pred.shape,x.shape,y.shape)


plt.plot(x,y, label=['line1','line2','line3','line4'])
plt.plot(x,y_pred)
plt.legend()
plt.ylim( -10,45)

import numpy as np

class SupportVectorRegressionMSE:
    def __init__(self, n_features, C=1.0, epsilon=0.1, lr=0.001, epochs=1000):
        self.W = np.zeros((n_features, 1))  # weights
        self.b = 0.0  # bias
        self.C = C  # Regularization parameter
        self.epsilon = epsilon  # Epsilon-tube
        self.lr = lr  # Learning rate
        self.epochs = epochs  # Number of training epochs

    def predict(self, X):
        """
        X: shape (num_samples, n_features)
        Returns: predictions, shape (num_samples, 1)
        """
        return np.dot(X, self.W) + self.b

    def epsilon_insensitive_loss(self, y_pred, y_true):
        """
        Epsilon-insensitive loss for SVR.
        """
        return np.maximum(0, np.abs(y_pred - y_true) - self.epsilon)

    def mean_squared_error(self, y_pred, y_true):
        """
        Standard MSE for performance evaluation.
        """
        m = y_true.shape[0]
        mse = np.sum((y_pred - y_true) ** 2) / m
        return mse

    def fit(self, X, y):
        """
        Fit the SVR model using a sub-gradient method with epsilon-insensitive loss.
        """
        m = X.shape[0]
        for epoch in range(self.epochs):
            y_pred = self.predict(X)
            loss = self.epsilon_insensitive_loss(y_pred, y)
            # Sub-gradient for SVR (linear)
            indicator = (np.abs(y_pred - y) > self.epsilon).astype(float) * np.sign(y_pred - y)
            dW = (1 / m) * np.dot(X.T, indicator) + self.C * self.W
            db = (1 / m) * np.sum(indicator)
            # Update weights and bias
            self.W -= self.lr * dW
            self.b -= self.lr * db
            # Optionally print MSE every 100 epochs
            if epoch % 100 == 0:
                mse = self.mean_squared_error(y_pred, y)
                print(f"Epoch {epoch}: MSE = {mse}")

# Example usage
if __name__ == "__main__":
    # Dummy data: 3 features
    X = np.array([[1, 2, 3],
                  [4, 5, 6],
                  [7, 8, 9],
                  [2, 3, 4]], dtype=float)
    y = np.array([[10],
                  [22],
                  [34],
                  [13]], dtype=float)

    model = SupportVectorRegressionMSE(n_features=3, C=0.01, epsilon=0.5, lr=0.001, epochs=1000)
    model.fit(X, y)
    print("Predictions:", model.predict(X))